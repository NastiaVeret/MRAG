{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "cec4915e89062d4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.7.0-cp313-cp313-win_amd64.whl.metadata (29 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.15.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Using cached pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from sentence_transformers) (4.13.2)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (80.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached numpy-2.2.5-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Using cached torch-2.7.0-cp313-cp313-win_amd64.whl (212.5 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.3-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached numpy-2.2.5-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, safetensors, regex, Pillow, numpy, networkx, joblib, fsspec, filelock, torch, scipy, huggingface-hub, tokenizers, scikit-learn, transformers, sentence_transformers\n",
      "Successfully installed Pillow-11.2.1 filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.31.1 joblib-1.5.0 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 sentence_transformers-4.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": "!pip install sentence_transformers",
   "id": "4c85d7d115dd0472"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\averet\\appdata\\local\\temp\\pip-req-build-em6rbtdx\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from clip==1.0) (2.7.0)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Using cached torchvision-0.22.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch->clip==1.0) (80.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torchvision->clip==1.0) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torchvision->clip==1.0) (11.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached torchvision-0.22.0-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369633 sha256=2c7beacb96abf2aa53d7b525d00b647152d585b6f71536a28522268c199ee5b7\n",
      "  Stored in directory: C:\\Users\\averet\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-uwv05al9\\wheels\\cb\\a8\\74\\5f32d6cf0407457f0f62737b6da5c14eb86b9cac476fdf630d\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, torchvision, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1 torchvision-0.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\averet\\AppData\\Local\\Temp\\pip-req-build-em6rbtdx'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": "!pip install git+https://github.com/openai/CLIP.git",
   "id": "795cf4c9fb59b226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.11.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from faiss-cpu) (2.2.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Using cached faiss_cpu-1.11.0-cp313-cp313-win_amd64.whl (15.0 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": "!pip install faiss-cpu",
   "id": "497004b5913976fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Using cached google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Using cached google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic (from google.generativeai)\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from google.generativeai) (4.13.2)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Using cached google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google.generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->google.generativeai)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->google.generativeai)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.4.26)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached grpcio-1.71.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: uritemplate, typing-inspection, pyparsing, pydantic-core, pyasn1, protobuf, grpcio, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-2.40.1 google-auth-httplib2-0.2.0 google.generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 pyparsing-3.2.3 rsa-4.9.1 typing-inspection-0.4.0 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": "!pip install google.generativeai",
   "id": "5d6a9b2292db87f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (2.7.0)\n",
      "Collecting pandas>=1.0.1 (from bert_score)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (4.51.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (2.2.5)\n",
      "Requirement already satisfied: requests in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (4.67.1)\n",
      "Collecting matplotlib (from bert_score)\n",
      "  Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.0.1->bert_score)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.0.1->bert_score)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from torch>=1.0.0->bert_score) (80.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.31.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert_score)\n",
      "  Using cached contourpy-1.3.2-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert_score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert_score)\n",
      "  Using cached fonttools-4.58.0-cp313-cp313-win_amd64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert_score)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from matplotlib->bert_score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->bert_score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from requests->bert_score) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\averet\\pycharmprojects\\multimodalrag\\.venv\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.2-cp313-cp313-win_amd64.whl (223 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, bert_score\n",
      "Successfully installed bert_score-0.3.13 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": "!pip install bert_score",
   "id": "88a6330e9cccc4e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:28:02.341344Z",
     "start_time": "2025-05-11T15:27:59.958777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import clip\n",
    "import faiss\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import evaluate"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract news from website",
   "id": "e745ed4aa681d299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:58:08.711932Z",
     "start_time": "2025-05-11T14:58:08.700399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_news = []\n",
    "os.makedirs(\"batch_issues/images\", exist_ok=True)"
   ],
   "id": "287944f5c7f22ad6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:29.969855Z",
     "start_time": "2025-05-11T14:59:17.547586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for issue in range(270, 300):\n",
    "    url = f'https://www.deeplearning.ai/the-batch/issue-{issue}/'\n",
    "    print(f'Processing: {url}')\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        news_section = soup.find(lambda tag: tag.name == \"h1\" and \"News\" in tag.text)\n",
    "        if not news_section:\n",
    "            print(f\"No 'News' section found in issue {issue}\")\n",
    "            continue\n",
    "\n",
    "        current = news_section.find_next_sibling()\n",
    "        while current:\n",
    "            if current.name == 'h1' and \"News\" in current.text:\n",
    "                break\n",
    "\n",
    "            if current.name == 'figure':\n",
    "                image_tag = current.find('img')\n",
    "                image_url = image_tag['src'] if image_tag else None\n",
    "\n",
    "                image_filename = f\"issue{issue}_img{len(all_news)}.jpg\"\n",
    "                image_path = f\"batch_issues/images/{image_filename}\"\n",
    "\n",
    "                if image_url:\n",
    "                    try:\n",
    "                        img_data = requests.get(image_url).content\n",
    "                        with open(image_path, 'wb') as img_file:\n",
    "                            img_file.write(img_data)\n",
    "                    except Exception as img_err:\n",
    "                        print(f\"Failed to download image: {img_err}\")\n",
    "                        image_path = \"No image\"\n",
    "\n",
    "                else:\n",
    "                    image_path = \"No image\"\n",
    "\n",
    "                title_tag = current.find_next_sibling()\n",
    "                while title_tag and title_tag.name != 'h1':\n",
    "                    title_tag = title_tag.find_next_sibling()\n",
    "                title = title_tag.text.strip() if title_tag else '---'\n",
    "\n",
    "                content_elements = []\n",
    "                next_tag = title_tag.find_next_sibling() if title_tag else None\n",
    "                while next_tag and next_tag.name not in ['figure', 'h1']:\n",
    "                    if next_tag.name in ['p', 'ul']:\n",
    "                        content_elements.append(next_tag.get_text(strip=True))\n",
    "                    next_tag = next_tag.find_next_sibling()\n",
    "\n",
    "                content = '\\n'.join(content_elements)\n",
    "\n",
    "                all_news.append({\n",
    "                    'issue': issue,\n",
    "                    'title': title,\n",
    "                    'image_file': image_path,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "                current = next_tag\n",
    "            else:\n",
    "                current = current.find_next_sibling()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Failed to fetch issue {issue}: {e}\")\n",
    "\n",
    "with open(\"batch_issues/news_270_to_299.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_news, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Done! News and images saved.\")"
   ],
   "id": "2d1a7bc25a4d6921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://www.deeplearning.ai/the-batch/issue-270/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-271/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-272/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-273/\n",
      "No 'News' section found in issue 273\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-274/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-275/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-276/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-277/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-278/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-279/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-280/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-281/\n",
      "No 'News' section found in issue 281\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-282/\n",
      "No 'News' section found in issue 282\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-283/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-284/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-285/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-286/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-287/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-288/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-289/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-290/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-291/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-292/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-293/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-294/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-295/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-296/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-297/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-298/\n",
      "Processing: https://www.deeplearning.ai/the-batch/issue-299/\n",
      "Done! News and images saved.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the documents into chunks",
   "id": "a7155eeb8f08a820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:37:58.412353Z",
     "start_time": "2025-05-11T15:37:58.402592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_path = Path(\"batch_issues/news_270_to_299.json\")\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    news_items = json.load(f)"
   ],
   "id": "22461240da8b26d",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:00.835347Z",
     "start_time": "2025-05-11T15:38:00.825877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")"
   ],
   "id": "26be4e2076265f2b",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:04.251045Z",
     "start_time": "2025-05-11T15:38:04.237738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_news = []\n",
    "for item in news_items:\n",
    "    title = item.get(\"title\", \"\")\n",
    "    content = item.get(\"content\", \"\")\n",
    "    image_file = item.get(\"image_file\", \"\")\n",
    "\n",
    "    chunks = text_splitter.split_text(content)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunked_news.append({\n",
    "            \"title\": title,\n",
    "            \"chunk\": chunk,\n",
    "            \"full_content\": item[\"content\"],\n",
    "            \"image_file\": image_file\n",
    "        })"
   ],
   "id": "f77bbe04dace35ae",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:07.211036Z",
     "start_time": "2025-05-11T15:38:07.200277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"chunked_news.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_news, f)"
   ],
   "id": "3c73041804f5ae26",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:09.090269Z",
     "start_time": "2025-05-11T15:38:09.083878Z"
    }
   },
   "cell_type": "code",
   "source": "len(chunked_news)",
   "id": "20b82b2d0ed0db80",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:11.708286Z",
     "start_time": "2025-05-11T15:38:11.702003Z"
    }
   },
   "cell_type": "code",
   "source": "chunked_news[0]",
   "id": "87a1d41e65211431",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Familiar Faces, Synthetic Soundtracks',\n",
       " 'chunk': 'Meta upped the ante for text-to-video generation with new systems that produce consistent characters and matching soundtracks.\\nWhat’s new:Meta presentedMovie Gen, a series of four systems that generate videos, include consistent characters, alter generated imagery, and add matching sound effects and music. Movie Gen will beavailableon Instagram in 2025. Meanwhile, you can view and listen to exampleshere. The teamexplainshow the model was built an extensive 92-page paper.',\n",
       " 'full_content': 'Meta upped the ante for text-to-video generation with new systems that produce consistent characters and matching soundtracks.\\nWhat’s new:Meta presentedMovie Gen, a series of four systems that generate videos, include consistent characters, alter generated imagery, and add matching sound effects and music. Movie Gen will beavailableon Instagram in 2025. Meanwhile, you can view and listen to exampleshere. The teamexplainshow the model was built an extensive 92-page paper.\\nGenerated videos:Movie Gen Video can output 256 frames (up to 16 seconds at 16 frames per second) at 1920x1080-pixel resolution. It includes a convolutional neural network autoencoder, transformer, and multiple embedding models.\\nMovie Gen Video produces imagery by flow matching, a technique related to diffusion. It learned to remove noise from noisy versions of images and videos given matching text descriptions from 1 billion image-text pairs and 100 million video-text pairs. At inference, it starts with pure noise and generates detailed imagery according to a text prompt.The system concatenates multiple text embeddings to combine the strengths of different embedding models.UL2was trained on text-only data, so its embeddings may provide “reasoning abilities,” according to the authors.Long-prompt MetaCLIPwas trained to produce similar text and image representations, so its embeddings might be useful for “cross-modal generation.”ByT5produces embeddings of individual text elements such as letters, numbers, and symbols; the system uses it when a prompt requests text within a clip.\\nConsistent characters:Given an image of a face, a fine-tuned version of Movie Gen Video generates a video that depicts a person with that face.\\nTo gather a training dataset for this capability, the team filtered Movie Gen Video’s pretraining dataset for clips that show a single face and consecutive frames are similar to one another. They built video-face examples by pairing each clip with a frame selected from the clip at random. To train the system, the team fed it text, the clip with added noise, and the single-frame face. It learned to remove the noise.Trained on this data alone, the system generated videos in which the person always faces the camera. To expand the variety of poses, they further trained it on examples that substituted the faces in the previous step withgenerated versionswith alternate poses and facial expressions.\\nAltered clips:The team modified Movie Gen Video’s autoencoder to accept an embedding of an alteration — say, changing the background or adding an object. They trained the system to alter videos in three stages:\\nFirst, they trained the system, given a starting image and an instruction to alter it, to produce an altered image.They further trained the system to produce altered clips. They generated two datasets of before-and-after clips based on instructions. (i) For instance, given a random frame and an instruction to, say, replace a person with a cat, the system altered the frame accordingly. Then the team subjected both frames to a series of augmentations selected at random, creating matching clips, one featuring a person, the other featuring a cat. Given the initial clip and the instruction, the system learned to generate the altered clip. (ii) The team usedDINOandSAM 2to segment clips. Given an unsegmented clip and an instruction such as “mark <object> with <color>,” the system learned to generate the segmented clip.Finally, they trained the system to restore altered clips to their original content. They built a dataset by taking a ground-truth clip and using their system to generate an altered version according to an instruction. Then Llama 3 rewrote the instruction to modify the altered clip to match the original. Given the altered clip and the instruction, the system learned to generate the original clip.\\nSynthetic soundtracks:Given a text description, a system called Movie Gen Audio generates sound effects and instrumental music for video clips up to 30 seconds long. It includes aDACVAEaudio encoder (which encodes sounds that comes before and/or after the target audio), Long-prompt MetaCLIP video encoder,T5text encoder, vanilla neural network that encodes the current time step, and transformer.\\nMovie Gen Audio learned to remove noise from noisy versions of audio associated with 1 million videos with text captions.\\xa0 At inference, it starts with pure noise and generates up to 30 seconds of audio at once.At inference, it can extend audio. Given the last n seconds of audio, the associated portion of a video, and a text description, it can generate the next 30 - n seconds.\\nResults:Overall, Movie Gen achieved performance roughly equal to or better than competitors in qualitative evaluations of overall quality and a number of specific qualities (such as “realness”). Human evaluators rated their preferences for Movie Gen or a competitor. The team reported the results in terms of net win rate (win percentage minus loss percentage) between -100 percent and 100 percent, where a score above zero means that a system won more than it lost.\\nFor overall video quality, Movie Gen achieved a net win rate of 35.02 percent versus Runway Gen3, 8.23 percent versus Sora (based on the prompts and generated clips available on OpenAI’s website), and 3.87 percent versus Kling 1.5.Generating clips of specific characters, Movie Gen achieved a net win rate of 64.74 percent versus ID-Animator, the state of the art for this capability.Generating soundtracks for videos from the SReal SFX dataset, Movie Gen Audio achieved a net win rate between 32 percent and 85 percent compared to various video-to-audio models.Altering videos in theTGVE+dataset, Movie Gen beat all competitors more than 70 percent of the time.\\nWhy it matters:With Movie Gen, table stakes for video generation rises to include consistent characters, soundtracks, and various video-to-video alterations. The 92-page paper is a valuable resource for builders of video generation systems, explaining in detail how the team filtered data, structured models, and trained them to achieve good results.\\nWe’re thinking:Meta has a great track record of publishing both model weights and papers that describe how the models were built. Kudos to the Movie Gen team for publishing the details of this work!',\n",
       " 'image_file': 'batch_issues/images/issue270_img0.jpg'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embeddings",
   "id": "737a8b614f5d601b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:35.174030Z",
     "start_time": "2025-05-11T15:38:21.805626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "texts = [item['title'] + '' + item['chunk'] for item in chunked_news]\n",
    "text_embeddings = model.encode(texts, convert_to_tensor=False)"
   ],
   "id": "66dee92b25aac1af",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:38:51.696959Z",
     "start_time": "2025-05-11T15:38:38.103848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "image_folder = Path(\"\")\n",
    "image_filenames = {item[\"image_file\"] for item in chunked_news if \"image_file\" in item}\n",
    "image_embeddings_dict = {}\n",
    "\n",
    "for filename in image_filenames:\n",
    "    try:\n",
    "        image_path = image_folder / filename\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_input = preprocess(image).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(image_input)\n",
    "            embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "            image_embeddings_dict[filename] = embedding.cpu()\n",
    "\n",
    "        print(f\" Processed {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Problem with {filename}: {e}\")\n",
    "        image_embeddings_dict[filename] = None\n"
   ],
   "id": "2fe083ce2a688ca0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed batch_issues/images/issue285_img52.jpg\n",
      " Processed batch_issues/images/issue270_img1.jpg\n",
      " Processed batch_issues/images/issue289_img66.jpg\n",
      " Processed batch_issues/images/issue271_img6.jpg\n",
      " Processed batch_issues/images/issue271_img7.jpg\n",
      " Processed batch_issues/images/issue288_img62.jpg\n",
      " Processed batch_issues/images/issue287_img60.jpg\n",
      " Processed batch_issues/images/issue290_img71.jpg\n",
      " Processed batch_issues/images/issue286_img55.jpg\n",
      " Processed batch_issues/images/issue277_img27.jpg\n",
      " Processed batch_issues/images/issue275_img17.jpg\n",
      " Processed batch_issues/images/issue287_img58.jpg\n",
      " Processed batch_issues/images/issue283_img42.jpg\n",
      " Processed batch_issues/images/issue298_img104.jpg\n",
      " Processed batch_issues/images/issue277_img25.jpg\n",
      " Processed batch_issues/images/issue284_img47.jpg\n",
      " Processed batch_issues/images/issue285_img50.jpg\n",
      " Processed batch_issues/images/issue279_img33.jpg\n",
      " Processed batch_issues/images/issue288_img64.jpg\n",
      " Processed batch_issues/images/issue290_img70.jpg\n",
      " Processed batch_issues/images/issue288_img63.jpg\n",
      " Processed batch_issues/images/issue292_img80.jpg\n",
      " Processed batch_issues/images/issue290_img72.jpg\n",
      " Processed batch_issues/images/issue292_img77.jpg\n",
      " Processed batch_issues/images/issue298_img105.jpg\n",
      " Processed batch_issues/images/issue279_img34.jpg\n",
      " Processed batch_issues/images/issue295_img91.jpg\n",
      " Processed batch_issues/images/issue274_img15.jpg\n",
      " Processed batch_issues/images/issue290_img69.jpg\n",
      " Processed batch_issues/images/issue275_img19.jpg\n",
      " Processed batch_issues/images/issue283_img43.jpg\n",
      " Processed batch_issues/images/issue298_img102.jpg\n",
      " Processed batch_issues/images/issue288_img61.jpg\n",
      " Processed batch_issues/images/issue296_img95.jpg\n",
      " Processed batch_issues/images/issue299_img106.jpg\n",
      " Processed batch_issues/images/issue291_img74.jpg\n",
      " Processed batch_issues/images/issue280_img38.jpg\n",
      " Processed batch_issues/images/issue283_img41.jpg\n",
      " Processed batch_issues/images/issue293_img84.jpg\n",
      " Processed batch_issues/images/issue272_img10.jpg\n",
      " Processed batch_issues/images/issue286_img56.jpg\n",
      " Processed batch_issues/images/issue276_img21.jpg\n",
      " Processed batch_issues/images/issue280_img39.jpg\n",
      " Processed batch_issues/images/issue295_img92.jpg\n",
      " Processed batch_issues/images/issue278_img31.jpg\n",
      " Processed batch_issues/images/issue272_img9.jpg\n",
      " Processed batch_issues/images/issue297_img99.jpg\n",
      " Processed batch_issues/images/issue287_img59.jpg\n",
      " Processed batch_issues/images/issue277_img28.jpg\n",
      " Processed batch_issues/images/issue294_img88.jpg\n",
      " Processed batch_issues/images/issue284_img46.jpg\n",
      " Processed batch_issues/images/issue299_img107.jpg\n",
      " Processed batch_issues/images/issue272_img11.jpg\n",
      " Processed batch_issues/images/issue297_img100.jpg\n",
      " Processed batch_issues/images/issue284_img45.jpg\n",
      " Processed batch_issues/images/issue286_img53.jpg\n",
      " Processed batch_issues/images/issue293_img81.jpg\n",
      " Processed batch_issues/images/issue285_img51.jpg\n",
      " Processed batch_issues/images/issue291_img73.jpg\n",
      " Processed batch_issues/images/issue289_img68.jpg\n",
      " Processed batch_issues/images/issue280_img40.jpg\n",
      " Processed batch_issues/images/issue297_img98.jpg\n",
      " Processed batch_issues/images/issue271_img4.jpg\n",
      " Processed batch_issues/images/issue295_img93.jpg\n",
      " Processed batch_issues/images/issue276_img23.jpg\n",
      " Processed batch_issues/images/issue272_img8.jpg\n",
      " Processed batch_issues/images/issue276_img22.jpg\n",
      " Processed batch_issues/images/issue294_img89.jpg\n",
      " Processed batch_issues/images/issue291_img75.jpg\n",
      " Processed batch_issues/images/issue299_img109.jpg\n",
      " Processed batch_issues/images/issue294_img86.jpg\n",
      " Processed batch_issues/images/issue287_img57.jpg\n",
      " Processed batch_issues/images/issue285_img49.jpg\n",
      " Processed batch_issues/images/issue284_img48.jpg\n",
      " Processed batch_issues/images/issue293_img83.jpg\n",
      " Processed batch_issues/images/issue291_img76.jpg\n",
      " Processed batch_issues/images/issue277_img26.jpg\n",
      " Processed batch_issues/images/issue278_img30.jpg\n",
      " Processed batch_issues/images/issue296_img96.jpg\n",
      " Processed batch_issues/images/issue299_img108.jpg\n",
      " Processed batch_issues/images/issue297_img101.jpg\n",
      " Processed batch_issues/images/issue271_img5.jpg\n",
      " Processed batch_issues/images/issue296_img94.jpg\n",
      " Processed batch_issues/images/issue270_img0.jpg\n",
      " Processed batch_issues/images/issue274_img14.jpg\n",
      " Processed batch_issues/images/issue296_img97.jpg\n",
      " Processed batch_issues/images/issue279_img36.jpg\n",
      " Processed batch_issues/images/issue275_img18.jpg\n",
      " Processed batch_issues/images/issue293_img82.jpg\n",
      " Processed batch_issues/images/issue278_img29.jpg\n",
      " Processed batch_issues/images/issue278_img32.jpg\n",
      " Processed batch_issues/images/issue289_img67.jpg\n",
      " Processed batch_issues/images/issue298_img103.jpg\n",
      " Processed batch_issues/images/issue286_img54.jpg\n",
      " Processed batch_issues/images/issue294_img87.jpg\n",
      " Processed batch_issues/images/issue295_img90.jpg\n",
      " Processed batch_issues/images/issue270_img3.jpg\n",
      " Processed batch_issues/images/issue270_img2.jpg\n",
      " Processed batch_issues/images/issue275_img16.jpg\n",
      " Processed batch_issues/images/issue280_img37.jpg\n",
      " Processed batch_issues/images/issue279_img35.jpg\n",
      " Processed batch_issues/images/issue289_img65.jpg\n",
      " Processed batch_issues/images/issue274_img12.jpg\n",
      " Processed batch_issues/images/issue276_img24.jpg\n",
      " Processed batch_issues/images/issue274_img13.jpg\n",
      " Processed batch_issues/images/issue292_img78.jpg\n",
      " Processed batch_issues/images/issue283_img44.jpg\n",
      " Processed batch_issues/images/issue292_img79.jpg\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FAISS Index",
   "id": "bcb288fa45a6034"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:04.109933Z",
     "start_time": "2025-05-11T15:39:04.102404Z"
    }
   },
   "cell_type": "code",
   "source": "text_embeddings_np = np.array(text_embeddings).astype(\"float32\")",
   "id": "c1705ba654a72a95",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:06.741189Z",
     "start_time": "2025-05-11T15:39:06.732116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"text_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_news, f)"
   ],
   "id": "1f4b96516d75579c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:08.703783Z",
     "start_time": "2025-05-11T15:39:08.694533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_index = faiss.IndexFlatL2(text_embeddings_np.shape[1])\n",
    "text_index.add(text_embeddings_np)\n",
    "faiss.write_index(text_index, \"text.index\")"
   ],
   "id": "c5a4a16a099fa3c1",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:10.641063Z",
     "start_time": "2025-05-11T15:39:10.635053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_embeddings_np = np.vstack([\n",
    "    emb.numpy() for emb in image_embeddings_dict.values() if emb is not None\n",
    "]).astype(\"float32\")"
   ],
   "id": "70c88cc1905aad87",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:13.023369Z",
     "start_time": "2025-05-11T15:39:13.015343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_index = faiss.IndexFlatL2(image_embeddings_np.shape[1])\n",
    "image_index.add(image_embeddings_np)\n",
    "faiss.write_index(image_index, \"image.index\")"
   ],
   "id": "bc26fcb3c5916334",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Multimodal Integration\n",
   "id": "df6dbbbb7b0e04ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:17.796108Z",
     "start_time": "2025-05-11T15:39:15.780428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api_key = os.getenv(\"API_KEY\")\n",
    "text_index = faiss.read_index(\"text.index\")\n",
    "with open(\"text_data.pkl\", \"rb\") as f:\n",
    "    news_items = pickle.load(f)\n",
    "\n",
    "image_folder = Path(\"\")\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "gemini = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")"
   ],
   "id": "66dd4db6e4a96161",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:30.674571Z",
     "start_time": "2025-05-11T15:39:30.669446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query]).astype(\"float32\")\n",
    "    distances, indices = text_index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        item = news_items[idx]\n",
    "        image_path = image_folder / item.get(\"image_file\", \"\")\n",
    "        results.append((item, image_path))\n",
    "    return results"
   ],
   "id": "17f3745c9f1e0f12",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:45.781309Z",
     "start_time": "2025-05-11T15:39:45.774721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_gemini(query, context_text):\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the question strictly based on the provided context.\n",
    "Do not invent or include any information that is not present in the context. If the answer cannot be found in the context, reply with: \"I don't know based on the given context.\"\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = gemini.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": 0.0}\n",
    "    )\n",
    "    return response.text.strip()\n"
   ],
   "id": "d90897bb1f33c70e",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "System Evaluation",
   "id": "e461feea153254d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To evaluate RAG system, I decided to check context relevance, groundedness and answer relevance. Why such metrics?\n",
    "\n",
    "- context relevance - because it is important to find relevant context. If the system find the wrong context, we can get the wrong answer for users\n",
    "\n",
    "- groundedness - RAG systems should rely on retrieved document\n",
    "\n",
    "- answer relevance - how well the generated answer addresses the original query"
   ],
   "id": "207f85e26d4707e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:50.092015Z",
     "start_time": "2025-05-11T15:39:50.083936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_answer(answer: str, reference: str):\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bert_result = bertscore.compute(predictions=[answer], references=[reference], lang=\"uk\")\n",
    "    bert_f1 = sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"])\n",
    "\n",
    "    return bert_f1\n"
   ],
   "id": "98e9ead7d2220afb",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:53.175468Z",
     "start_time": "2025-05-11T15:39:53.168933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_context_relevance(query, docs):\n",
    "    query_emb = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    docs_emb = embedding_model.encode(docs, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_emb, docs_emb)\n",
    "    return float(similarity.mean())"
   ],
   "id": "7d55747f5ffd9c19",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:39:54.931392Z",
     "start_time": "2025-05-11T15:39:54.925125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_groundedness(answer, docs):\n",
    "    answer_emb = embedding_model.encode(answer, convert_to_tensor=True)\n",
    "    docs_emb = embedding_model.encode(docs, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(answer_emb, docs_emb)\n",
    "    return float(similarity.max())"
   ],
   "id": "54878a5200aae83c",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:41:23.538245Z",
     "start_time": "2025-05-11T15:41:12.892806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_query = input()\n",
    "results = search(user_query, top_k=3)\n",
    "\n",
    "item, image_path = results[0]\n",
    "full_context = f\"{item['title']}\\n\\n{item['full_content']}\"\n",
    "answer = ask_gemini(user_query, full_context)\n",
    "\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)\n",
    "print(\"\\n Article:\")\n",
    "print(f\" Title: {item['title']}\")\n",
    "print(f\" Chunk: {item['chunk']}\")\n",
    "print(f\" Content: {item['full_content']}\")\n",
    "if image_path.exists():\n",
    "    print(f\" Image: {image_path}\")\n",
    "else:\n",
    "    print(\"Image not found.\")\n",
    "\n",
    "context_relevance = compute_context_relevance(user_query, [item[\"chunk\"]])\n",
    "groundedness = compute_groundedness(answer, [item[\"chunk\"]])\n",
    "bert_f1 = evaluate_answer(answer, item['chunk'])\n",
    "\n",
    "print(f\"\\n Evaluation Metrics:\")\n",
    "print(f\" Context Relevance: {context_relevance:.3f}\")\n",
    "print(f\" Groundedness: {groundedness:.3f}\")\n",
    "print(f\" Answer Relevance: {bert_f1:.4f}\")\n"
   ],
   "id": "5374b14c4ff7c8d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer:\n",
      "Google launched Lyria 1 in 2023.\n",
      "\n",
      " Article:\n",
      " Title: Music Generation for Pros\n",
      " Chunk: Behind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.\n",
      " Content: Google refreshed its experimental tools for composers and producers.\n",
      "What’s new:Google announced updates of two music-generation apps and the models they're based on.Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlisthere.MusicFX DJgenerates a continuous stream of music that users can modify as it plays. Try it outhere.\n",
      "How it works:The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details.\n",
      "Music AI Sandbox is based on the updatedLyria 2music generator. It lets users generate new clips, roughly 30 seconds long, according to prompts. Users can enter lyrics, extend existing clips, and rearrange segments with generated transitions, introductions, and endings.MusicFX DJ, which is based on a different model calledLyria RealTime, lets users control streaming music via prompts and other settings. Users can change or combine genres, add or subtract instruments, change key, and speed up or slow down without interrupting the stream.\n",
      "Behind the news:GooglelaunchedLyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently becameavailablevia the Vertex API to developers who are preapproved by Google.\n",
      "Why it matters:While music generators likeSuno and Udioappeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe hasempoweredvideographers and Runway haspartneredwith movie producers.\n",
      "We’re thinking:API access to Lyria 2 would be music to our ears!\n",
      " Image: batch_issues\\images\\issue299_img107.jpg\n",
      "\n",
      " Evaluation Metrics:\n",
      " Context Relevance: 0.587\n",
      " Groundedness: 0.604\n",
      " Answer Relevance: 0.7073\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
